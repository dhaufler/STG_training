{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 09:07:28.656210: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-17 09:07:28.726426: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-17 09:07:28.726456: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-17 09:07:28.726462: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-17 09:07:28.730939: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# load_sparse.py: modifying for STG\n",
    "import os\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np\n",
    "from time import time\n",
    "import warnings\n",
    "from load_sparse import create_network_dat\n",
    "# from memory_profiler import profile\n",
    "\n",
    "def sort_indices(indices, *arrays):\n",
    "    max_ind = np.max(indices) + 1\n",
    "    if np.iinfo(indices.dtype).max < max_ind * (max_ind + 1) :\n",
    "        indices = indices.astype(np.int64)\n",
    "    q = indices[:, 0] * max_ind + indices[:, 1]\n",
    "    sorted_ind = np.argsort(q)\n",
    "    sorted_arrays = list(map(lambda arr: arr[sorted_ind], [indices, *arrays]))\n",
    "    return tuple(sorted_arrays)\n",
    "\n",
    "# The following function, although it provided a speed up, it creates bunch of tensor copies \n",
    "# which are not garbage collected and thus it is not memory efficient\n",
    "def sort_indices_tf(indices, *arrays):\n",
    "    indices = tf.cast(indices, dtype=tf.int64)    \n",
    "    max_ind = tf.reduce_max(indices) + 1\n",
    "    q = indices[:, 0] * max_ind + indices[:, 1]\n",
    "    sorted_ind = tf.argsort(q)\n",
    "    indices = tf.cast(indices, dtype=tf.int32)\n",
    "    sorted_arrays = [tf.gather(arr, sorted_ind).numpy() for arr in [indices, *arrays]]\n",
    "    return tuple(sorted_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load main network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../GLIF_network/network_dat.pkl\"\n",
    "h5_path=\"../GLIF_network/network/stg_nodes.h5\"\n",
    "core_only=False\n",
    "n_neurons=685\n",
    "seed=3000\n",
    "connected_selection=False\n",
    "n_syn_basis=5\n",
    "tensorflow_speed_up=False\n",
    "\n",
    "\n",
    "rd = np.random.RandomState(seed=seed)\n",
    "\n",
    "# Create / Load the network_dat pickle file from the SONATA files\n",
    "if not os.path.exists(path):\n",
    "    base_dir = os.path.dirname(path)\n",
    "    data_dir = os.path.join(base_dir, \"network\")\n",
    "    print(f\"Creating {path} file...\")\n",
    "    d = create_network_dat(data_dir=data_dir, source='stg', target='stg',\n",
    "                            output_file=path, save_pkl=True)\n",
    "    # d = create_network_dat(data_dir='GLIF_network/network', source='v1', target='v1',\n",
    "    #                         output_file='GLIF_network/network_dat.pkl', save_pkl=True)\n",
    "else:\n",
    "    print(\"Loading network_dat.pkl file...\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        d = pkl.load(f)  # d is a dictionary with 'nodes' and 'edges' keys\n",
    "\n",
    "# Get the number of nodes in the full V1 network\n",
    "n_nodes = sum([len(a[\"ids\"]) for a in d[\"nodes\"]])  # 296991 total neurons\n",
    "# Create arrays to convert between bmtk and tf ides\n",
    "tf_id_to_bmtk_id = np.arange(n_nodes, dtype=np.int32)\n",
    "bmtk_id_to_tf_id = np.full(n_nodes, -1, dtype=np.int32)\n",
    "\n",
    "# Extract from the SONATA file the nodes information\n",
    "# h5_file = h5py.File(h5_path, \"r\")            \n",
    "node_data_dict = {}\n",
    "with h5py.File(h5_path, \"r\") as h5_file:\n",
    "    assert np.diff(h5_file[\"nodes\"][\"stg\"][\"node_id\"]).var() < 1e-12\n",
    "    node_type_id = np.array(h5_file['nodes']['stg']['node_type_id'], dtype=np.int32)\n",
    "            \n",
    "    # Dynamically get individual node specific parameters:\n",
    "    # allows for the addition/removal of node parameters during the bmtk build stage\n",
    "    # without changing this portion of the training code\n",
    "    node_keys = list(h5_file[\"nodes\"][\"stg\"][\"0\"].keys())\n",
    "    node_group = h5_file[\"nodes\"][\"stg\"][\"0\"]\n",
    "    for key in node_keys:\n",
    "        if node_group[key].dtype == 'float64' or node_group[key].dtype == 'float32':\n",
    "            dtype = np.float32\n",
    "        elif node_group[key].dtype == 'int64' or node_group[key].dtype == 'int32':\n",
    "            dtype = np.int32\n",
    "        else:\n",
    "            dtype = str\n",
    "        \n",
    "        data = np.array(node_group[key], dtype=dtype)\n",
    "        # Clean up key name\n",
    "        safe_key = ''.join(c if c.isalnum() or c == '_' else '_' for c in key)\n",
    "        node_data_dict[safe_key] = data\n",
    "        \n",
    "r = np.sqrt(node_data_dict['x']**2 + node_data_dict['z']**2)  # the maximum radius is 845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### CHOOSE THE NETWORK NODES ###\n",
    "if n_neurons > n_nodes:  # used to be an explicit number: 296991\n",
    "    raise ValueError(f\"There are only {n_nodes} neurons in the network\")\n",
    "\n",
    "elif connected_selection: # this condition takes the n_neurons closest neurons to the origin\n",
    "    sorted_ind = np.argsort(r)\n",
    "    sel = np.zeros(n_nodes, dtype=np.bool_)\n",
    "    sel[sorted_ind[:n_neurons]] = True\n",
    "    print(f\"> Maximum sample radius: {r[sorted_ind[n_neurons - 1]]:.2f}\")\n",
    "\n",
    "elif core_only: # this condition takes the n_neurons closest neurons to the origin (core)\n",
    "    sel = r < 400\n",
    "    n_core = np.sum(sel)\n",
    "    if n_neurons > n_core:\n",
    "        raise ValueError(f\"There are only {n_core} neurons in the network core\")\n",
    "    elif n_neurons > 0 and n_neurons <= n_core:\n",
    "        (inds,) = np.where(sel)\n",
    "        take_inds = rd.choice(inds, size=n_neurons, replace=False)\n",
    "        sel[:] = False\n",
    "        sel[take_inds] = True\n",
    "\n",
    "elif n_neurons > 0 and n_neurons <= n_nodes: # this condition randomly selects neurons from the whole STG\n",
    "    print(n_neurons)\n",
    "    print(n_nodes)\n",
    "    legit_neurons = np.arange(n_nodes)\n",
    "    take_inds = rd.choice(legit_neurons, size=n_neurons, replace=False)\n",
    "    \n",
    "    # Check to make sure at least one edge exists between the chosen nodes --> will cause an error later if not\n",
    "    # Shouldn't be an issue if the network is connected but is a bug for feedforward versions\n",
    "    edges = d[\"edges\"]\n",
    "    for edge in edges:\n",
    "        # Identify which of the 10 types of inputs we have\n",
    "        edge_type_id = edge[\"edge_type_id\"]\n",
    "        target_tf_ids = bmtk_id_to_tf_id[edge[\"target\"]]\n",
    "        source_tf_ids = bmtk_id_to_tf_id[edge[\"source\"]]\n",
    "        edge_exists = np.logical_and(target_tf_ids != -1, source_tf_ids != -1)\n",
    "        # find the edges between the selected nodes\n",
    "        target_tf_ids = target_tf_ids[edge_exists]\n",
    "        source_tf_ids = source_tf_ids[edge_exists]\n",
    "    # If no edges exist between the chosen nodes, modify the node selection to include a single edge   \n",
    "    if target_tf_ids.size == 0 and len(edges) > 0:\n",
    "        warnings.warn(\"Due to sparse edges no edges exist between the chosen nodes\")\n",
    "        warnings.warn(\"Modifying node selection to include a single edge to create a largely feedforward network.\")\n",
    "        # get the source and target nodes IDs of a single edge   \n",
    "        edge = edges[0]\n",
    "        target_id = edge[\"target\"][0]\n",
    "        source_id = edge[\"source\"][0]\n",
    "        # Check if both source and target nodes are in the selected nodes & replace a selected node if not\n",
    "        if target_id not in take_inds:            \n",
    "            take_inds[0] = target_id           \n",
    "        if source_id not in take_inds:\n",
    "            take_inds[1] = source_id\n",
    "            \n",
    "    # !!! np.empty creates an array with a RANDOM amount of True & False values !!!\n",
    "    # sel = np.empty(n_nodes, dtype=np.bool_)\n",
    "    sel = np.zeros(n_nodes, dtype=np.bool_) # Creates an array with ONLY False values\n",
    "    sel[take_inds] = True # Assigns True to the randomly selected neurons\n",
    "\n",
    "else: # if no condition is met, all neurons are selected\n",
    "    sel = np.ones(n_nodes, dtype=np.bool_)\n",
    "\n",
    "# Get the number of neurons in the chosen network and update the traslation arrays\n",
    "n_nodes = np.sum(sel)\n",
    "print(f\"> Number of Neurons: {n_nodes}\")\n",
    "tf_id_to_bmtk_id = tf_id_to_bmtk_id[sel] # tf idx '0' corresponds to 'tf_id_to_bmtk_id[0]' bmtk idx\n",
    "bmtk_id_to_tf_id[tf_id_to_bmtk_id] = np.arange(n_nodes, dtype=np.int32) \n",
    "# bmtk idx '0' corresponds to 'bmtk_id_to_tf_id[0]' tf idx which can be '-1' in case\n",
    "# the bmtk node is not in the tensorflow selection or another value in case it belongs the selection\n",
    "\n",
    "# for tf_id, bmtk_id in enumerate(tf_id_to_bmtk_id):\n",
    "#     bmtk_id_to_tf_id[bmtk_id] = tf_id\n",
    "\n",
    "# Get the properties from the network neurons\n",
    "#tuning_angle = tuning_angle[sel]\n",
    "node_type_id = node_type_id[sel]\n",
    "\n",
    "# Get the target cell type properties from the network neurons    \n",
    "for key, data in node_data_dict.items():\n",
    "    node_data_dict[key] = data[sel]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET THE NODES PARAMETERS\n",
    "n_node_types = len(d[\"nodes\"])\n",
    "node_params = dict(\n",
    "    V_th=np.empty(n_node_types, np.float32),\n",
    "    g=np.empty(n_node_types, np.float32),\n",
    "    E_L=np.empty(n_node_types, np.float32),\n",
    "    k=np.empty((n_node_types, 2), np.float32),\n",
    "    C_m=np.empty(n_node_types, np.float32),\n",
    "    V_reset=np.empty(n_node_types, np.float32),\n",
    "    t_ref=np.empty(n_node_types, np.float32),\n",
    "    asc_amps=np.empty((n_node_types, 2), np.float32),\n",
    ")\n",
    "\n",
    "node_type_ids = np.empty(n_nodes, np.int32)\n",
    "for i, node_type in enumerate(d[\"nodes\"]):\n",
    "    # get ALL the nodes of the given node type\n",
    "    tf_ids = bmtk_id_to_tf_id[np.array(node_type[\"ids\"], dtype=np.int32)]\n",
    "    # choose only those that belong to our model\n",
    "    tf_ids = tf_ids[tf_ids >= 0]\n",
    "    # assign them all the same id (which does not relate with the neuron type)\n",
    "    node_type_ids[tf_ids] = i\n",
    "    for k, v in node_params.items():\n",
    "        # save in a dict the information of the nodes\n",
    "        v[i] = node_type[\"params\"][k]\n",
    "\n",
    "# GET THE EDGES INFORMATION\n",
    "t0 = time()\n",
    "edges = d[\"edges\"]\n",
    "n_edges = 0\n",
    "dense_shape = (n_nodes, n_nodes)\n",
    "indices = []\n",
    "weights = []\n",
    "delays = []\n",
    "syn_ids = []\n",
    "edge_type_ids = []\n",
    "\n",
    "for edge in edges:\n",
    "    # Identify which of the 10 types of inputs we have\n",
    "    edge_type_id = edge[\"edge_type_id\"]\n",
    "    target_tf_ids = bmtk_id_to_tf_id[edge[\"target\"]]\n",
    "    source_tf_ids = bmtk_id_to_tf_id[edge[\"source\"]]\n",
    "    edge_exists = np.logical_and(target_tf_ids != -1, source_tf_ids != -1)\n",
    "    # select the edges within our model\n",
    "    target_tf_ids = target_tf_ids[edge_exists]\n",
    "    source_tf_ids = source_tf_ids[edge_exists]\n",
    "    weights_tf = edge[\"params\"][\"weight\"][edge_exists].astype(np.float32)\n",
    "\n",
    "    n_new_edge = len(target_tf_ids)\n",
    "    n_edges += int(n_new_edge)\n",
    "    \n",
    "    # all the edges of a given type have the same delay and synaptic id\n",
    "    delays_tf = np.full(n_new_edge, edge[\"params\"][\"delay\"], dtype=np.float16)\n",
    "    syn_id = np.full(n_new_edge, edge[\"params\"][\"syn_id\"], dtype=np.uint8)\n",
    "\n",
    "    indices.append(np.array([target_tf_ids, source_tf_ids]).T)\n",
    "    weights.append(weights_tf)\n",
    "    delays.append(delays_tf)\n",
    "    syn_ids.append(syn_id)\n",
    "    edge_type_ids.append(np.full(n_new_edge, edge_type_id, dtype=np.uint16))\n",
    "\n",
    "print(f\"> Number of Synapses: {n_edges}\")\n",
    "indices = np.concatenate(indices, axis=0, dtype=np.int32)\n",
    "weights = np.concatenate(weights, axis=0, dtype=np.float32)\n",
    "delays = np.concatenate(delays, axis=0, dtype=np.float16)\n",
    "syn_ids = np.concatenate(syn_ids, axis=0, dtype=np.uint8)\n",
    "edge_type_ids = np.concatenate(edge_type_ids, axis=0, dtype=np.uint16)\n",
    "\n",
    "# sort indices by considering first all the targets of node 0, then all of node 1, ...\n",
    "# indices, weights, delays, tau_syn_weights_array, syn_ids = sort_indices(indices, weights, delays, tau_syn_weights_array, syn_ids)\n",
    "# indices, weights, delays, syn_ids = sort_indices_tf(indices, weights, delays, syn_ids)\n",
    "# indices, weights, delays, syn_ids = sort_indices_tf(indices, weights, delays, syn_ids)\n",
    "\n",
    "if tensorflow_speed_up:\n",
    "    indices, weights, delays, syn_ids, edge_type_ids = sort_indices_tf(indices, weights, delays, syn_ids, edge_type_ids)\n",
    "else:\n",
    "    indices, weights, delays, syn_ids, edge_type_ids = sort_indices(indices, weights, delays, syn_ids, edge_type_ids)\n",
    "\n",
    "network = dict(\n",
    "    #tuning_angle=tuning_angle,\n",
    "    node_type_id=node_type_id,\n",
    "    n_nodes=n_nodes,\n",
    "    n_edges=n_edges,\n",
    "    node_params=node_params,\n",
    "    node_type_ids=node_type_ids,\n",
    "    synapses=dict(indices=indices, weights=weights, delays=delays, \n",
    "                    syn_ids=syn_ids, edge_type_ids=edge_type_ids,\n",
    "                    dense_shape=dense_shape),\n",
    "    tf_id_to_bmtk_id=tf_id_to_bmtk_id,\n",
    "    bmtk_id_to_tf_id=bmtk_id_to_tf_id,\n",
    ")\n",
    "\n",
    "# Add the node data to the network dictionary\n",
    "network.update(node_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load input networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network_dat.pkl file...\n",
      "Number nodes available:  685\n",
      "Number neurons to select:  685\n",
      "> Number of Neurons: 685\n",
      "> Number of Synapses: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/allen/programs/mindscope/workgroups/realistic-model/elena.westeinde/human_model/STG_training/v1_model_utils/load_sparse.py:277: UserWarning: Due to sparsity of edges no edges exist between the chosen nodes\n",
      "  warnings.warn(\"Due to sparsity of edges no edges exist between the chosen nodes\")\n",
      "/allen/programs/mindscope/workgroups/realistic-model/elena.westeinde/human_model/STG_training/v1_model_utils/load_sparse.py:278: UserWarning: Modifying node selection to include a single edge to create a largely feedforward network.\n",
      "  warnings.warn(\"Modifying node selection to include a single edge to create a largely feedforward network.\")\n"
     ]
    }
   ],
   "source": [
    "import load_sparse\n",
    "\n",
    "network = load_sparse.load_network(\n",
    "                    path=\"../GLIF_network/network_dat.pkl\",\n",
    "                    h5_path=\"../GLIF_network/network/stg_nodes.h5\",\n",
    "                    core_only=False,\n",
    "                    n_neurons=685,\n",
    "                    seed=3000,\n",
    "                    connected_selection=False,\n",
    "                    n_syn_basis=5,\n",
    "                    tensorflow_speed_up=False\n",
    "                    )\n",
    "\n",
    "data_dir=\"../GLIF_network/network\"\n",
    "lgn_path=\"../GLIF_network/aud_input_dat.pkl\"\n",
    "bkg_path=\"../GLIF_network/bkg_input_dat.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=1\n",
    "bmtk_id_to_tf_id=network[\"bmtk_id_to_tf_id\"] \n",
    "tensorflow_speed_up=False\n",
    "\n",
    "lgn_path = os.path.join(data_dir, \"aud_input_dat.pkl\")\n",
    "bkg_path = os.path.join(data_dir, \"bkg_input_dat.pkl\")\n",
    "# LOAD THE LGN INPUT\n",
    "if not os.path.exists(lgn_path):\n",
    "    # print(\"Creating lgn_input_dat.pkl file...\")\n",
    "    print(f\"Creating {lgn_path} file...\")\n",
    "    # Process LGN input network\n",
    "    lgn_input = create_network_dat(data_dir=data_dir, source='aud_input', target='stg',\n",
    "                                    output_file=lgn_path, save_pkl=True)\n",
    "    # lgn_input = create_network_dat(data_dir='GLIF_network/network', source='lgn', target='v1', \n",
    "    #                                 output_file=lgn_path, save_pkl=True)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    with open(lgn_path, \"rb\") as f:\n",
    "        lgn_input = pkl.load(f)\n",
    "        \n",
    "# create_network_dat specifically does not load node info for the LGN and BKG inputs --> because the nodes have no trainable parameters?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE BACKGROUND INPUT\n",
    "if not os.path.exists(bkg_path):\n",
    "    # print(\"Creating bkg_input_dat.pkl file...\")\n",
    "    print(f\"Creating {bkg_path} file...\")\n",
    "    # Process LGN input network\n",
    "    bkg_input = create_network_dat(data_dir=data_dir, source='bkg', target='stg',\n",
    "                                    output_file=bkg_path, save_pkl=True)\n",
    "    # bkg_input = create_network_dat(data_dir='GLIF_network/network', source='bkg', target='v1', \n",
    "    #                                 output_file=bkg_path, save_pkl=True)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    with open(bkg_path, \"rb\") as f:\n",
    "        bkg_input = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4451\n",
      "685\n",
      "true\n",
      "[ 557 2113 4077 ... 2952  239 3261]\n"
     ]
    }
   ],
   "source": [
    "input_edges = [lgn_input['edges'], bkg_input['edges']]\n",
    "for edge in lgn_input['edges']:\n",
    "    target_bmtk_id = edge[\"target\"]\n",
    "    print(max(target_bmtk_id))\n",
    "    print(len(bmtk_id_to_tf_id))\n",
    "    if max(target_bmtk_id) >= len(bmtk_id_to_tf_id):\n",
    "        print('true')\n",
    "        keep_idx = target_bmtk_id < len(bmtk_id_to_tf_id)\n",
    "        target_bmtk_id = target_bmtk_id[keep_idx]\n",
    "        source_tf_id = edge[\"source\"]\n",
    "        weights_tf = edge[\"params\"][\"weight\"]\n",
    "#         print(\"Warning: Input target nodes exceed the number of nodes in the network, reducing target node IDs\")\n",
    "\n",
    "# print(bkg_input['edges'])\n",
    "# print(bkg_input['edges'][0][\"target\"])\n",
    "# print(bkg_input['edges'][0][\"source\"])\n",
    "\n",
    "print(source_tf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Input target nodes exceed the number of nodes in the network, reducing node IDs\n"
     ]
    }
   ],
   "source": [
    "# Unite the edges information of the LGN and the background noise\n",
    "input_edges = [lgn_input['edges'], bkg_input['edges']]\n",
    "    \n",
    "input_populations = []\n",
    "for idx, input_population in enumerate(input_edges):\n",
    "    post_indices = []\n",
    "    pre_indices = []\n",
    "    weights = []\n",
    "    delays = []\n",
    "    syn_ids = []\n",
    "\n",
    "    for edge in input_population:  # input_population[1]:\n",
    "        # Identify the which of the 10 types of inputs we have\n",
    "        # r = edge[\"params\"][\"receptor_type\"] - 1\n",
    "        # r takes values within 0 - 9\n",
    "        target_bmtk_id = edge[\"target\"]\n",
    "        source_tf_id = edge[\"source\"]\n",
    "        weights_tf = edge[\"params\"][\"weight\"]\n",
    "        \n",
    "        if max(target_bmtk_id) >= len(bmtk_id_to_tf_id):\n",
    "            keep_idx = target_bmtk_id < len(bmtk_id_to_tf_id)\n",
    "            target_bmtk_id = target_bmtk_id[keep_idx]\n",
    "            source_tf_id = source_tf_id[keep_idx]\n",
    "            weights_tf = weights_tf[keep_idx]\n",
    "            print(\"Warning: Input target nodes exceed the number of nodes in the tf network, reducing input node IDs\")\n",
    "        \n",
    "        # delays_tf = np.array(edge[\"params\"][\"delay\"], dtype=np.float16)\n",
    "        # syn_id = np.array(edge[\"params\"][\"syn_id\"], dtype=np.uint8)\n",
    "        # delays_tf = (np.zeros_like(weights_tf) + edge[\"params\"][\"delay\"]).astype(np.float16)\n",
    "        # syn_id = (np.zeros_like(weights_tf) + edge[\"params\"][\"syn_id\"]).astype(np.uint8)\n",
    "\n",
    "        n_new_edge = len(target_bmtk_id)\n",
    "        delays_tf = np.full(n_new_edge, edge[\"params\"][\"delay\"], dtype=np.float16)\n",
    "        syn_id = np.full(n_new_edge, edge[\"params\"][\"syn_id\"], dtype=np.uint8)\n",
    "\n",
    "        if bmtk_id_to_tf_id is not None:\n",
    "            # check if the given edges exist in our model\n",
    "            # (notice that only the target must exist since the source is within the LGN module)\n",
    "            # This means that source index is within 0-17400\n",
    "            target_tf_id = bmtk_id_to_tf_id[target_bmtk_id]\n",
    "            edge_exists = target_tf_id != -1\n",
    "            target_tf_id = target_tf_id[edge_exists]\n",
    "            source_tf_id = source_tf_id[edge_exists]\n",
    "            weights_tf = weights_tf[edge_exists]\n",
    "            delays_tf = delays_tf[edge_exists]\n",
    "            syn_id = syn_id[edge_exists]\n",
    "\n",
    "            # we multiply by 10 the indices and add r to identify the receptor_type easily:\n",
    "            # if target id is divisible by 10 the receptor_type is 0,\n",
    "            # if it is rest is 1 by dividing by 10 then its receptor type is 1, and so on...\n",
    "            # extend acts by extending the list with the given object\n",
    "            # new_target_tf_id = target_tf_id * max_n_receptors + r\n",
    "            new_target_tf_id = target_tf_id\n",
    "            post_indices.append(new_target_tf_id)\n",
    "            pre_indices.append(source_tf_id)\n",
    "            weights.append(weights_tf)\n",
    "            delays.append(delays_tf)\n",
    "            syn_ids.append(syn_id)\n",
    "\n",
    "    post_indices = np.concatenate(post_indices, axis=0, dtype=np.int32)\n",
    "    pre_indices = np.concatenate(pre_indices, axis=0, dtype=np.int32)\n",
    "    weights = np.concatenate(weights, axis=0, dtype=np.float32)\n",
    "    delays = np.concatenate(delays, axis=0, dtype=np.float16)\n",
    "    syn_ids = np.concatenate(syn_ids, axis=0, dtype=np.uint8)\n",
    "\n",
    "    # first column are the post indices and second column the pre indices\n",
    "    indices = np.stack([post_indices, pre_indices], axis=-1)\n",
    "\n",
    "    # Sort indices\n",
    "    # indices, weights, delays, syn_ids = sort_indices_tf(indices, weights, delays, syn_ids)\n",
    "    if tensorflow_speed_up:\n",
    "        indices, weights, delays, syn_ids = sort_indices_tf(indices, weights, delays, syn_ids)\n",
    "    else:\n",
    "        indices, weights, delays, syn_ids = sort_indices(indices, weights, delays, syn_ids)\n",
    "\n",
    "    if idx == 0:\n",
    "        # we load the LGN nodes and their positions\n",
    "        lgn_nodes_h5_file = h5py.File(os.path.join(data_dir,\"aud_input_nodes.h5\"), \"r\")\n",
    "        #lgn_nodes_h5_file = h5py.File(f\"{data_dir}/network/lgn_nodes.h5\", \"r\")\n",
    "        n_inputs = len(lgn_nodes_h5_file[\"nodes\"][\"aud_input\"][\"node_id\"])\n",
    "    else:\n",
    "        # we load the background nodes and their positions\n",
    "        bkg_nodes_h5_file = h5py.File(os.path.join(data_dir,\"bkg_nodes.h5\"), \"r\")\n",
    "        #bkg_nodes_h5_file = h5py.File(f\"{data_dir}/network/bkg_nodes.h5\", \"r\")\n",
    "        n_inputs = len(bkg_nodes_h5_file[\"nodes\"][\"bkg\"][\"node_id\"])\n",
    "\n",
    "    input_populations.append(\n",
    "        dict(\n",
    "            n_inputs=n_inputs,\n",
    "            indices=indices,\n",
    "            weights=weights,\n",
    "            delays=delays,\n",
    "            syn_ids=syn_ids,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"../GLIF_network/network\"\n",
    "lgn_path=\"../GLIF_network/aud_input_dat.pkl\"\n",
    "bkg_path=\"../GLIF_network/bkg_input_dat.pkl\"\n",
    "dt=1\n",
    "bmtk_id_to_tf_id=network[\"bmtk_id_to_tf_id\"] \n",
    "tensorflow_speed_up=False\n",
    "\n",
    "inputs = load_sparse.load_input(\n",
    "                    start=1000, \n",
    "                    duration=1000,\n",
    "                    dt=1,\n",
    "                    data_dir=\"../GLIF_network/network\",\n",
    "                    lgn_path=\"../GLIF_network/aud_input_dat.pkl\",\n",
    "                    bkg_path=\"../GLIF_network/bkg_input_dat.pkl\",\n",
    "                    bmtk_id_to_tf_id=network[\"bmtk_id_to_tf_id\"], \n",
    "                    tensorflow_speed_up=False\n",
    ")\n",
    "    \n",
    "    \n",
    "lgn_input = inputs[0]\n",
    "bkg_input = inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_input_population(input_population, new_n_input, seed=3000):\n",
    "    rd = np.random.RandomState(seed=seed)\n",
    "\n",
    "    in_ind = input_population[\"indices\"]\n",
    "    in_weights = input_population[\"weights\"]\n",
    "    in_delays = input_population[\"delays\"]\n",
    "    in_syn_ids = input_population[\"syn_ids\"]\n",
    "\n",
    "    # we take input_population['n_inputs'] neurons from a list of new_n_input with replace,\n",
    "    # which means that in the end there can be less than new_n_input neurons of the LGN,\n",
    "    # but they are randonmly selected\n",
    "    assignment = rd.choice(np.arange(new_n_input, dtype=np.int32), size=input_population[\"n_inputs\"], replace=True)\n",
    "\n",
    "    # Create a tuple of indices for vectorized operations\n",
    "    post_indices = in_ind[:, 0]\n",
    "    input_neuron_indices = in_ind[:, 1]\n",
    "    assigned_neuron_indices = assignment[input_neuron_indices]\n",
    "\n",
    "    # Calculate unique pairs and their indices in the original array\n",
    "    unique_pairs, inverse_indices = np.unique(np.stack((post_indices, assigned_neuron_indices), axis=1), axis=0, return_inverse=True)\n",
    "\n",
    "    # Accumulate weights for repeated pairs\n",
    "    new_in_weights = np.bincount(inverse_indices, weights=in_weights)\n",
    "\n",
    "    # Lets get the average delay by the connection strength (synaptic weight), \n",
    "    # assuming stronger connections might have a more significant impact on the timing.\n",
    "    # Calculate mean delays for each unique pair\n",
    "    # First, accumulate the total delays for each unique pair\n",
    "    total_delays = np.bincount(inverse_indices, weights=in_delays)\n",
    "    # Count the occurrences of each unique pair to divide and get the mean\n",
    "    counts = np.bincount(inverse_indices)\n",
    "    # Calculate the mean by dividing total delays by counts\n",
    "    new_in_delays = total_delays / counts\n",
    "\n",
    "    # Similarly for the syn_ids\n",
    "    new_in_syn_ids = np.bincount(inverse_indices, weights=in_syn_ids.astype(np.int32))\n",
    "    new_in_syn_ids = (new_in_syn_ids/counts).astype(np.uint8)\n",
    "\n",
    "    # new_in_ind, new_in_weights, new_in_delays = sort_input_indices(\n",
    "    #     new_in_ind, new_in_weights, new_in_delays\n",
    "    # )\n",
    "\n",
    "    new_in_ind, new_in_weights, new_in_delays, new_in_syn_ids = sort_indices(\n",
    "        unique_pairs, new_in_weights, new_in_delays, new_in_syn_ids\n",
    "    )\n",
    "    \n",
    "    new_input_population = dict(\n",
    "        n_inputs=new_n_input,\n",
    "        indices=new_in_ind.astype(np.int32),\n",
    "        weights=new_in_weights.astype(np.float32),\n",
    "        delays=new_in_delays.astype(np.float16),\n",
    "        syn_ids=new_in_syn_ids,\n",
    "        # spikes=None,\n",
    "    )\n",
    "\n",
    "    return new_input_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_inputs': 500,\n",
       " 'indices': array([[  0,   7],\n",
       "        [  0,  10],\n",
       "        [  0,  24],\n",
       "        ...,\n",
       "        [684, 495],\n",
       "        [684, 496],\n",
       "        [684, 498]], dtype=int32),\n",
       " 'weights': array([0.05, 0.05, 0.05, ..., 0.05, 0.05, 0.05], dtype=float32),\n",
       " 'delays': array([1.7, 1.7, 1.7, ..., 1.7, 1.7, 1.7], dtype=float16),\n",
       " 'syn_ids': array([62, 62, 62, ..., 62, 62, 62], dtype=uint8)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgn_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If required reduce the number of LGN inputs\n",
    "n_input = 500\n",
    "fseed = 3000\n",
    "if n_input != 17400:\n",
    "    lgn_input = reduce_input_population(lgn_input, n_input, seed=fseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_inputs': 500,\n",
       " 'indices': array([[  0,   0],\n",
       "        [  0,  16],\n",
       "        [  0,  18],\n",
       "        ...,\n",
       "        [684, 481],\n",
       "        [684, 491],\n",
       "        [684, 493]], dtype=int32),\n",
       " 'weights': array([0.05, 0.05, 0.1 , ..., 0.05, 0.05, 0.1 ], dtype=float32),\n",
       " 'delays': array([1.7, 1.7, 1.7, ..., 1.7, 1.7, 1.7], dtype=float16),\n",
       " 'syn_ids': array([62, 62, 62, ..., 62, 62, 62], dtype=uint8)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgn_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
