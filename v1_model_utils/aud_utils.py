import os
import h5py
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import random

def count_nodes(file_path, population_name=None):
    """
    Count the number of nodes in a specific population of a Sonata file.
    
    Parameters:
        file_path (str): Path to the Sonata file.
        population_name (str, optional): Specific population name to search for.
    Returns:
        int: Number of nodes in the specified population or total nodes if None.
    """
    try:
        with h5py.File(file_path, 'r') as h5file:
            if 'nodes' in h5file:
                nodes_group = h5file['nodes']
                if population_name:
                    if population_name in nodes_group:
                        population_group = nodes_group[population_name]
                        if 'node_id' in population_group:
                            return len(population_group['node_id'])
                        else:
                            print(f"'node_id' not found in population '{population_name}'.")
                            return 0
                    else:
                        print(f"Population '{population_name}' not found in the file.")
                        return 0
                else:
                    total_nodes = 0
                    for pop_name in nodes_group.keys():
                        population_group = nodes_group[pop_name]
                        if 'node_id' in population_group:
                            total_nodes += len(population_group['node_id'])
                    return total_nodes
            else:
                print("No 'nodes' group found in the file.")
                return 0
    except Exception as e:
        print(f"An error occurred while reading the file '{file_path}': {e}")
        return 0
    

def roll_out(_x, extractor_model, state_variables, flags):
    """
    Simulate one forward pass (rollout) through the model with dummy inputs.

    1. **Function Purpose**:
        - Simulates the neural activity (spikes and voltage) over a given sequence length using the model.
        - Updates the internal state of the model after the forward pass.

    2. **Inputs**:
        - `_x`: The primary input tensor, representing the LGN (auditory input) activity.
        - `seq_len`: The sequence length for the simulation, determining the number of timesteps to process.

    3. **Process**:
        - **Initial State Setup**:
            - Reads the initial state variables (`state_variables`) to set the starting conditions for the RSNN cell.
        - **Dummy Background Inputs**:
            - Creates a placeholder tensor (`dummy_zeros`) for background inputs. This tensor has zeros for all timesteps and neurons and matches the specified batch size and sequence length.
        - **Forward Pass**:
            - Performs a forward pass through the `extractor_model`, which outputs the spikes, voltage, and updated internal state for the sequence.
        - **Output Extraction**:
            - Extracts:
            - `_z`: Spikes generated by the neurons during the sequence.
            - `_v`: Voltage levels over the sequence.
            - Updates the state variables to the new state (`new_state`) produced by the forward pass.

    4. **Output**:
        - Returns a dictionary containing:
            - `"spikes"`: Spike outputs from the RSNN over time.
            - `"voltage"`: Voltage levels of the neurons over time.

    5. **Applications**:
        - Useful for testing and validating the modelâ€™s behavior with specific inputs.
        - Can be used to generate data for plotting or further analysis of neural activity.

    Args:
        _x: Input tensor (LGN input).
        seq_len: Sequence length.

    Returns:
        Dictionary with spikes and voltage outputs.
    """
    # Read the initial state
    _initial_state = tf.nest.map_structure(lambda a: a.read_value(), state_variables)
    
    # Create dummy zeros for background inputs
    dummy_zeros = tf.zeros((flags.batch_size, flags.seq_len, flags.neurons), dtype=tf.float32)

    # Perform a forward pass through the extractor model
    _out, _p = extractor_model((_x, dummy_zeros, _initial_state))

    # Extract spikes and voltage
    _z, _v, _ = _out[0]  # Spikes (_z), Voltage (_v)

    # Update state variables with the new state
    new_state = tuple(_out[1:])
    tf.nest.map_structure(lambda a, b: a.assign(b), state_variables, new_state)

    # Return outputs
    return {
        "spikes": _z,
        "voltage": _v,
    }

def randomize_weights(model, input_type, shape, scale, zero_percentage=0.0):
    """
    Randomizes the specified input weights using a lognormal distribution,
    sets a percentage of weights to zero, and updates the model with the new weights.

    Parameters:
    - model: The Keras model containing the input layers.
    - input_type: 'aud_in' for auditory inputs or 'bkg' for background inputs.
    - shape: The `shape` parameter for the lognormal distribution (lognormal standard deviation).
    - scale: The `scale` parameter for the lognormal distribution (exponentiated mean).
    - zero_percentage: The percentage of weights to set to zero (0 to 100).

    Returns:
    - median_weight: The median of the randomized weights.
    """
    # Determine the layer to update
    if input_type == "aud_in":
        layer = model.get_layer("input_layer")
        layer_name = "aud_input"
    elif input_type == "bkg":
        layer = model.get_layer("noise_layer")
        layer_name = "bkg_input"
    else:
        raise ValueError("Invalid input_type. Choose 'aud_in' or 'bkg'.")

    # Get the current weights
    weights = layer.get_weights()

    # Randomize the weights using lognormal distribution
    randomized_weights = np.random.lognormal(mean=np.log(scale), sigma=shape, size=weights[0].shape)

    # Set a percentage of weights to zero
    if zero_percentage > 0.0:
        num_zero_weights = int(len(randomized_weights) * (zero_percentage / 100))
        zero_indices = np.random.choice(len(randomized_weights), num_zero_weights, replace=False)
        randomized_weights[zero_indices] = 0

    weights[0] = randomized_weights  # Update weights

    # Write the randomized weights back to the model
    layer.set_weights(weights)
    print(f"Updated {layer_name} weights with lognormal distribution (shape={shape}, scale={scale}, zero_percentage={zero_percentage}).")

    # Calculate and return the median of the randomized weights
    median_weight = np.median(randomized_weights[randomized_weights > 0])  # Exclude zeros from median calculation
    print(f"Median weight for {layer_name} (excluding zeros): {median_weight}")
    return median_weight


def load_aud_input_and_target_rates(StimID, base_dir, target_rates_dir, StimID_to_StimName, seq_len, n_input, batch_size, t_start):
    """
    Loads auditory input spikes and corresponding target firing rates for a given StimID.

    Parameters:
    - StimID (int): The stimulus ID used to identify the spikes file and target rates.
    - base_dir (str): Base directory containing the auditory input spikes.
    - target_rates_dir (str): Directory containing the target rate CSV files.
    - StimID_to_StimName (dict): Mapping of stimulus numbers to target identifiers (ignores sorted index).
    - seq_len (int): Sequence length (in ms).
    - n_input (int): Number of auditory input neurons.
    - batch_size (int): Batch size for data loading.
    - t_start (float): Start time for the sequence (in ms).

    Returns:
    - aud_in_spikes (np.ndarray): Auditory input spikes [batch_size, seq_len, n_input].
    - target_rates (np.ndarray): Target firing rates [batch_size, seq_len, num_cells].
    - recorded_cell_ids (list): List of 'recorded_cell_id' numbers corresponding to target rates.
    - sorted_indices (list): List of sorted index values extracted from file names.
    - chosen_dir (str): Directory of the selected stimulus for debugging purposes.
    """
    # Map StimID to StimName
    if StimID not in StimID_to_StimName:
        raise ValueError(f"StimID {StimID} not found in StimID_to_StimName mapping.")
    StimName = StimID_to_StimName[StimID]

    # Find a matching directory for the given StimID
    sub_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(f"stim_{StimID}")]
    if not sub_dirs:
        raise ValueError(f"No directories found for StimID {StimID} in {base_dir}.")

    # Randomly select a subdirectory
    chosen_dir = os.path.join(base_dir, random.choice(sub_dirs))
    spike_file = os.path.join(chosen_dir, 'spikes.csv')
    if not os.path.exists(spike_file):
        raise FileNotFoundError(f"'spikes.csv' not found in folder: {chosen_dir}")

    # Load auditory input spikes
    spike_data = pd.read_csv(spike_file, delim_whitespace=True)
    min_time = spike_data['timestamps'].min()
    max_time = spike_data['timestamps'].max()
    duration = max_time - min_time

    if duration <= seq_len:
        raise ValueError(f"Stimulus duration {duration} is less than or equal to sequence length {seq_len}")

    # Generate batch_size start times evenly spaced over the stimulus duration
    start_times = np.linspace(min_time + t_start, max_time - seq_len, batch_size)

    aud_in_spikes = np.zeros((batch_size, seq_len, n_input), dtype=int)

    for batch_idx, interval_start in enumerate(start_times):
        interval_end = interval_start + seq_len
        selected_spikes = spike_data[(spike_data['timestamps'] >= interval_start) & (spike_data['timestamps'] < interval_end)]

        for _, row in selected_spikes.iterrows():
            time_idx = int(row['timestamps'] - interval_start)
            neuron_id = int(row['node_ids'])
            if 0 <= neuron_id < n_input and 0 <= time_idx < seq_len:
                aud_in_spikes[batch_idx, time_idx, neuron_id] = 1

    # Load target rates
    target_rate_files = [f for f in os.listdir(target_rates_dir) if f.split('__')[1].startswith(StimName)]
    if not target_rate_files:
        raise ValueError(f"No target rate files found for StimName {StimName} in {target_rates_dir}.")

    # Sort files and extract recorded_cell_ids and sorted indices
    target_rate_files.sort()
    # recorded_cell_ids = [f.split('__')[0] for f in target_rate_files]
    # sorted_indices = [int(f.split('__')[1].split('_')[-1][2:].replace('.csv', '')) for f in target_rate_files]
    recorded_cell_ids = [f.split('__')[0] for f in target_rate_files]
    sorted_indices = [int(f.split('__')[1].split('_si')[-1].replace('.csv', '')) for f in target_rate_files]

    # Prepare target rates array
    num_cells = len(recorded_cell_ids)
    target_rates = np.zeros((batch_size, seq_len, num_cells), dtype=float)

    for cell_idx, rate_file in enumerate(target_rate_files):
        rate_data = pd.read_csv(os.path.join(target_rates_dir, rate_file))

        for batch_idx, interval_start in enumerate(start_times):
            interval_end = interval_start + seq_len
            selected_rates = rate_data[(rate_data['Time'] >= interval_start) & (rate_data['Time'] < interval_end)]

            if len(selected_rates) < seq_len:
                # Handle padding if necessary
                padding = np.zeros(seq_len - len(selected_rates))
                selected_rates = np.concatenate([selected_rates['Rate'].values, padding])
            elif len(selected_rates) > seq_len:
                selected_rates = selected_rates.iloc[:seq_len]['Rate'].values
            else:
                selected_rates = selected_rates['Rate'].values

            target_rates[batch_idx, :, cell_idx] = selected_rates

    return aud_in_spikes, target_rates, recorded_cell_ids, sorted_indices, chosen_dir


def plot_target_rates_and_input_spikes(target_rates, aud_in_spikes, sorted_indices=None):
    """
    Plots a 2D heatmap of target firing rates and a raster plot of auditory input spikes.

    Parameters:
    - target_rates: np.ndarray of shape [batch_size, seq_len, num_target_cells]
    - aud_in_spikes: np.ndarray of shape [batch_size, seq_len, num_input_units]
    - sorted_indices: Optional list of sorted indices for reordering target rates
    """
    # Ensure batch size is 1
    if target_rates.ndim != 3 or target_rates.shape[0] != 1:
        raise ValueError("Target rates must have shape [1, seq_len, num_target_cells].")
    if aud_in_spikes.ndim != 3 or aud_in_spikes.shape[0] != 1:
        raise ValueError("Auditory input spikes must have shape [1, seq_len, num_input_units].")

    # Remove batch dimension
    target_rates = target_rates[0]  # Shape: [seq_len, num_target_cells]
    aud_in_spikes = aud_in_spikes[0]  # Shape: [seq_len, num_input_units]

    # Debugging shapes
    print("Target Rates Shape:", target_rates.shape)
    print("Auditory Input Spikes Shape:", aud_in_spikes.shape)

    # Reorder target rates based on sorted_indices if provided
    if sorted_indices is not None:
        if len(sorted_indices) != target_rates.shape[1]:
            raise ValueError("Length of sorted_indices must match the number of target cells.")
        sorted_order = np.argsort(sorted_indices)
        target_rates = target_rates[:, sorted_order]

    # Scale target rates by 1000
    target_rates *= 1000

    # Clip target rates at the 90th percentile
    clip_value = np.percentile(target_rates, 90)
    target_rates = np.clip(target_rates, 0, clip_value)

    # Transpose for plotting (neurons on Y-axis, time on X-axis)
    target_rates = target_rates.T  # Shape: [num_target_cells, seq_len]
    aud_in_spikes = aud_in_spikes.T  # Shape: [num_input_units, seq_len]

    # Check spike distribution for auditory input
    spike_neurons, spike_times = np.where(aud_in_spikes > 0)  # Corrected order
    print("Total auditory input spikes:", len(spike_times))
    print("Spike times snippet:", spike_times[:10])
    print("Spike neurons snippet:", spike_neurons[:10])

    # Create the figure and subplots
    fig, axes = plt.subplots(1, 2, figsize=(15, 10), constrained_layout=False)

    # Plot target rates heatmap
    im1 = axes[0].imshow(target_rates, aspect="auto", cmap="Greys", origin="lower",
                          extent=[0, target_rates.shape[1], 0, target_rates.shape[0]])
    axes[0].set_title("Target Firing Rates Heatmap")
    axes[0].set_xlabel("Time (ms)")
    axes[0].set_ylabel("Target Cell Index")
    fig.colorbar(im1, ax=axes[0], label="Firing Rate (Hz)")

    # Plot auditory input spikes raster
    axes[1].scatter(spike_times, spike_neurons, c="black", s=1, label="Spikes")  # Corrected order
    axes[1].set_title("Auditory Input Spikes Raster")
    axes[1].set_xlim(0, aud_in_spikes.shape[1])  # Match x-axis to heatmap
    axes[1].set_ylim(0, aud_in_spikes.shape[0])  # Match y-axis to number of input units
    axes[1].set_xlabel("Time (ms)")
    axes[1].set_ylabel("Input Unit Index")

    # Optional: Limit the y-axis for target rates for better visualization
   # axes[0].set_ylim(0, 300)

    # Show the plot
    plt.show()
    return fig, axes

def plot_voltage_and_spikes_from_rollout(rollout_results):
    """
    Plots a 2D heatmap of the voltage outputs and a raster plot of spikes from rollout results.

    Parameters:
    - rollout_results: Dictionary containing 'voltage' and 'spikes' with shapes 
      [batch_size, seq_len, n_neurons].
    """
    # Extract voltage and spikes
    voltage = rollout_results['voltage']
    spikes = rollout_results['spikes']

    # Ensure inputs are numpy arrays
    voltage = voltage.numpy() if hasattr(voltage, "numpy") else voltage
    spikes = spikes.numpy() if hasattr(spikes, "numpy") else spikes

    # Ensure batch size is 1
    if voltage.ndim != 3 or voltage.shape[0] != 1:
        raise ValueError("Voltage and spikes must have shape [1, seq_len, n_neurons].")

    # Remove batch dimension
    voltage = voltage[0]  # Shape: [seq_len, n_neurons]
    spikes = spikes[0]    # Shape: [seq_len, n_neurons]

    # Debugging shapes
    print("Voltage shape after batch removal:", voltage.shape)
    print("Spikes shape after batch removal:", spikes.shape)

    # Transpose for plotting (neurons on Y-axis, time on X-axis)
    voltage = voltage.T  # Shape: [n_neurons, seq_len]
    spikes = spikes.T    # Shape: [n_neurons, seq_len]

    # Debugging shapes after transpose
    print("Voltage shape after transpose:", voltage.shape)
    print("Spikes shape after transpose:", spikes.shape)

    # Check spike distribution
    spike_neurons, spike_times = np.where(spikes > 0)  # Corrected order
    print("Total spikes:", len(spike_times))
    print("Spike times snippet:", spike_times[:10])
    print("Spike neurons snippet:", spike_neurons[:10])

    # Create the figure and subplots
    fig, axes = plt.subplots(1, 2, figsize=(15, 10), constrained_layout=False)

    # Plot voltage heatmap
    im1 = axes[0].imshow(voltage, aspect="auto", cmap="viridis", origin="lower",
                          extent=[0, voltage.shape[1], 0, voltage.shape[0]])
    axes[0].set_title("Voltage Heatmap")
    axes[0].set_xlabel("Time (ms)")
    axes[0].set_ylabel("Neuron Index")
    #fig.colorbar(im1, ax=axes[0], label="Voltage (a.u.)")

    # Plot spikes raster
    axes[1].scatter(spike_times, spike_neurons, c="black", s=1, label="Spikes")  # Corrected order
    axes[1].set_title("Spike Raster")
    axes[1].set_xlim(0, voltage.shape[1])  # Match x-axis to voltage heatmap
    axes[1].set_ylim(0, voltage.shape[0])  # Match y-axis to voltage heatmap
    axes[1].set_xlabel("Time (ms)")
    axes[1].set_ylabel("Neuron Index")

    #axes[0].set_xlim([0, 500])
    #axes[1].set_xlim([0, 500])
    #axes[0].set_ylim([0, 100])
    #axes[1].set_ylim([0, 100])
    # Show the plot
    plt.show()
    
    return fig, axes